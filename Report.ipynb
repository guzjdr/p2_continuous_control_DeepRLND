{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "296bf5e5",
   "metadata": {},
   "source": [
    "## Project 2 - Continuous Control \n",
    "\n",
    "\n",
    "**Controlling the two-link arm reacher**\n",
    "\n",
    "Project Goals:\n",
    "\n",
    "* Implement the TD3 Algorithm to control the two link arm reacher ( unity environment ).\n",
    "* The agent is able to receive an average reward of +13 (or higher) over a 100 episode epoch.\n",
    "\n",
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: ./data_files/actor_network.png \"Actor_Network_Arch\"\n",
    "[image2]: ./data_files/critic_network.png \"Critic_Network_Arch\"\n",
    "\n",
    "[single-agent]: ./data_files/exp0_single_agent.png \"EXP0_Single_EXP\"\n",
    "[image3]: ./data_files/exp0_single_agent_results.png \"EXP0_Single_Agent_Graph\"\n",
    "\n",
    "[update-critic-loss]: ./data_files/critic_loss_function.png \"EXP0_Multi_EXP\"\n",
    "[multi-agent]: ./data_files/exp0_mutli_agent_training.png \"EXP0_Multi_EXP\"\n",
    "[image4]: ./data_files/exp0_multi_agent_score_graph.png \"EXP0_Multi_Agent_Graph\"\n",
    "[image5]: ./data_files/exp0_multi_agent_eval_results.png \"EXP0_Multi_Agent_Eval\"\n",
    "\n",
    "[image6]: ./data_files/exp1_multi_agent_scores_graph.png \"EXP1_Multi_Agent_Graph\"\n",
    "[image7]: ./data_files/exp1_multi_agent_eval_result.png \"EXP1_Multi_Agent_Eval\"\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Report\n",
    "\n",
    "\n",
    "### Learning Algorithm\n",
    "#### 1. Description\n",
    "I've implemented the Twin Delay DDPG (TD3) Algorithm to solve the task described above. The TD3 Algorithm is an extension of the Vanilla DDPG Algorithm that was introduced in this Nano-Degree.\n",
    "\n",
    "The TD3 differs from the original DDPG Algorithm in three distinct ways:\n",
    "\n",
    "1. TD3 learns from two separate target critic networks (Q1_target, Q2_target). Hence the \"Twin\" in Twin Delay DDPG \n",
    "\n",
    "2. The local critic network updates more frequently than the local actor network and the target networks. It is recommended to use a 2:1 update frequency ratio - i.e. update critic network 2 times for every actor / target network update. Hence the \"Delay\" in Twin Delay DDPG\n",
    "\n",
    "3. Addition of noise to target actions with the intend to stabilize the local critic network.  \n",
    "\n",
    " \n",
    "\n",
    "For further information please refer to OpenAI's Spinning Up documentation here: [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/algorithms/td3.html)\n",
    "\n",
    "<u><b>Modules:</b></u>\n",
    "- <u>Replay Buffer</u>: Used to store and collect experience tuples (state, action, reward, next_state, done). \n",
    "- <u>Agent</u>: Agent class containing act, step, learn, and soft_update functions.  \n",
    "- <u>ModelsQ</u>: Definition of the Deep Neural Net Architecture using Pytorch.\n",
    "\n",
    "\n",
    "#### 2. Final Set of Hyper-parameters for EXP 1:\n",
    "```python\n",
    "#Module Variables\n",
    "#Replay Buffer\n",
    "BUFFER_SIZE = int(1e5) # memory replay buffer size\n",
    "BATCH_SIZE = 64 # batch size\n",
    "#Q Network Hyper-parameters\n",
    "GAMMA = 0.99 # Q learning discount size\n",
    "TAU = 1e-3 # for soft update from local network to target network\n",
    "LR_ACTOR = 5e-4 # learning rate\n",
    "LR_CRITIC = 5e-4 # learning rate\n",
    "#Update frequencies\n",
    "UPDATE_CRITIC_EVERY = 10 # number of frames used to update the local network\n",
    "UPDATE_ACTOR_TARGET = 2 * UPDATE_CRITIC_EVERY\n",
    "NN_NUM_UPDATES = 10\n",
    "\n",
    "#Noise Parameters\n",
    "NOISE_FACTOR = 0.8\n",
    "NOISE_MIN_MAX = 0.5\n",
    "```\n",
    "\n",
    "#### 3. Final Model Architectures\n",
    "\n",
    "\n",
    "![alt text][image1]<center>**Figure 1**</center>\n",
    "\n",
    "\n",
    "![alt text][image2]<center>**Figure 2**</center>\n",
    "\n",
    "\n",
    "\n",
    "### Plot of Rewards\n",
    "<p></p>\n",
    "\n",
    "<center><b>In this report, I show the results of the VERY FIRST AND LAST SET of experiments:</b></center>\n",
    "\n",
    "\n",
    "Please refer to the ./data_files/EXPS_ARCHIVE to view the initial set of experiments - these experiments show the learning trend, but fail to correctly illustrate the score function. \n",
    "\n",
    "### Single Agent Experiments\n",
    "\n",
    "This experiment was a first attempt at solving the task at hand. As shown below, the agent was not able to learn much from the training environment. The actor-critic update frequency was set to 1:2 respectively. \n",
    "\n",
    "![alt text][single-agent]<center>**Figure 3**</center>\n",
    "![alt text][image3]<center>**Figure 4**</center>\n",
    "\n",
    "\n",
    "### Multi Agent Experiments\n",
    "\n",
    "After further experiments - experimenting with the hyper-parameters: network update frequencies, noise factor values,  different model architectures - attempts, the following results where acquired:\n",
    "\n",
    "Note: the critic loss function was updated from previous experiments (EXPS_ARCHIVE used the loss function shown in red):\n",
    "\n",
    "![alt text][update-critic-loss]<center>**Figure 6**</center>\n",
    "![alt text][multi-agent]<center>**Figure 6**</center>\n",
    "\n",
    "#### EXP 0:\n",
    "![alt text][image4]<center>**Figure 7**</center>\n",
    "![alt text][image5]<center>**Figure 8**</center>\n",
    "\n",
    "\n",
    "In the final experiment, I ran the training for a longer number of periods. In this final experiment the agent was able to learn much faster than the previous experiment. \n",
    "\n",
    "#### EXP 1:\n",
    "![alt text][image6]<center>**Figure 9**</center>\n",
    "![alt text][image7]<center>**Figure 10**</center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Ideas for Future Work\n",
    "\n",
    "#### TD3:\n",
    "- Further explore the different combinations of hyper-parameters.\n",
    "- Implement a PPO algorithm, and compare it to this implementation.\n",
    "- Run the last experiment - EXP1 - multiple times to see if the Q function converges every time. \n",
    "- Implement more complex Q-functions.\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
