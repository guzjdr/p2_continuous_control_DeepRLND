{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "296bf5e5",
   "metadata": {},
   "source": [
    "## Project 2 - Continuous Control \n",
    "\n",
    "\n",
    "**Controlling the two link arm reacher**\n",
    "\n",
    "Project Goals:\n",
    "\n",
    "* Implement a TD3 Algorithm to control the Unity two link arm reacher.\n",
    "* The agent is able to receive an average reward of +13 (or higher) over a 100 episode epoch.\n",
    "\n",
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: ./p1_files/DQNetwork_training_results.png \"DQN_Score_Results\"\n",
    "[image2]: ./p1_files/DuelingNetwork_training_results.png \"DuelingN_Score_Results\"\n",
    "[image3]: ./p1_files/ModelArchitecture.png \"Model_Architecture\"\n",
    "[image4]: ./p1_files/DuelingArchitecture.png \"Model_Architecture\"\n",
    "[image5]: ./p1_files/Dueling_Network.png \"Dueling_Network_Architecture\"\n",
    "---\n",
    "\n",
    "### Report\n",
    "\n",
    "\n",
    "### Learning Algorithm\n",
    "#### 1. Description\n",
    "A modularized implementation of the vanilla DQN Algorithm provided as part of the Deep Q-Network lesson. \n",
    "Note: very minor modifications were made to solve this problem. \n",
    "\n",
    "<u><b>Modules:</b></u>\n",
    "- <u>Replay Buffer</u>: Used to store and collect experience tuples (state, action, reward, next_state, done). \n",
    "- <u>Agent</u>: Agent class containing action, step, learn, and soft_update functions.  \n",
    "- <u>ModelsQ</u>: Definition of the Deep Neural Net Architecture using Pytorch.\n",
    "\n",
    "\n",
    "#### 2. Hyperparameters\n",
    "```python\n",
    "#Module Variables\n",
    "BUFFER_SIZE = int(1e5) # memory replay buffer size\n",
    "BATCH_SIZE = 64 # batch size\n",
    "GAMMA = 0.99 # Q learning discount size\n",
    "TAU = 1e-3 # for soft update from local network to target network\n",
    "LR = 5e-4 # learning rate \n",
    "UPDATE_EVERY = 4 # number of frames used to update the local network\n",
    "```\n",
    "\n",
    "#### 3. Model Architecture\n",
    "The DQN Model Architecture consist of one input layer, two inner layers, and an output layer as illustrated in Figure 1 below. \n",
    "\n",
    "![alt text][image3]<center>**Figure 1**</center>\n",
    "\n",
    "The Dueling Network Architecture was adapted as shown in Figures 2 and 3 below. \n",
    "\n",
    "\n",
    "\n",
    "![alt text][image4]<center>**Figure 2**</center>\n",
    "![alt text][image5]<center>**Figure 3**</center>\n",
    "\n",
    "Note: This was an initial attempt.\n",
    "\n",
    "### Plot of Rewards\n",
    "\n",
    "#### 1. DQN Score Results\n",
    "The agent was able to receive an average reward of +13 (or higher) over a 100 episode epoch beyond step 600. \n",
    "\n",
    "![alt text][image1]\n",
    "\n",
    "\n",
    "#### 2. DuelingN Score Results\n",
    "The agent was never able to receive an average reward of +13 (or higher) over a 100 episode epoch.\n",
    "\n",
    "![alt text][image2]\n",
    "\n",
    "\n",
    "\n",
    "### Ideas for Future Work\n",
    "\n",
    "#### 1. DQN Vanilla Implementation:\n",
    "- Implement the Prioritized Experience Replay Buffer\n",
    "- Use the epsilon-greedy action selection (exploration v.s. exploitation) hyperparameter\n",
    " \n",
    "#### 2. Dueling Network Implementation:\n",
    "\n",
    "- Further investigate how to properly implement this algorithm beyond this first initial attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc26c46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
